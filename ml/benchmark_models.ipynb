{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Benchmarking Algorithmes ML - Consommation Electrique\n\nComparaison de 3 algorithmes pour choisir le meilleur pour la prediction.\n\n**Algorithmes compares:**\n1. Linear Regression (baseline simple)\n2. Gradient Boosting (ensemble avance)\n3. Random Forest (ensemble robuste)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Chargement donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger donnees enrichies 3 sources\n",
    "data_path = \"../data/conso_enrichi_3sources.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(f\"Donnees chargees: {len(df)} enregistrements\")\n",
    "print(f\"Periode: {df['datetime'].min()} -> {df['datetime'].max()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Preparation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features multi-sources\n",
    "features = [\n",
    "    'heure',\n",
    "    'jour_semaine',\n",
    "    'mois',\n",
    "    'jour_mois',\n",
    "    'est_weekend',\n",
    "    'prix_spot_eur_mwh',\n",
    "    'est_ferie',\n",
    "    'est_vacances'\n",
    "]\n",
    "\n",
    "target = 'mw_conso'\n",
    "\n",
    "# Nettoyer et preparer\n",
    "df_clean = df.dropna(subset=[target] + features)\n",
    "\n",
    "X = df_clean[features]\n",
    "y = df_clean[target]\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Features: {features}\")\n",
    "print(f\"\\nTrain set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## 4. Definition des algorithmes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# 3 algorithmes a comparer\nmodels = {\n    'Linear Regression': LinearRegression(),\n    \n    'Gradient Boosting': GradientBoostingRegressor(\n        n_estimators=100,\n        max_depth=5,\n        learning_rate=0.1,\n        random_state=42\n    ),\n    \n    'Random Forest': RandomForestRegressor(\n        n_estimators=200,\n        max_depth=15,\n        min_samples_split=5,\n        min_samples_leaf=2,\n        random_state=42,\n        n_jobs=-1\n    )\n}\n\nprint(f\"Algorithmes a benchmarker: {list(models.keys())}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Entrainement et evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "results = []\n\nprint(\"=\" * 70)\nprint(\"   BENCHMARKING EN COURS\")\nprint(\"=\" * 70 + \"\\n\")\n\nfor name, model in models.items():\n    print(f\"\\nAlgorithme: {name}\")\n    print(\"-\" * 50)\n    \n    # Entrainement\n    start_time = time.time()\n    model.fit(X_train, y_train)\n    train_time = time.time() - start_time\n    \n    # Cross-validation\n    cv_scores = cross_val_score(\n        model, X_train, y_train,\n        cv=5, scoring='neg_mean_absolute_error'\n    )\n    cv_mae = -cv_scores.mean()\n    \n    # Predictions\n    y_pred = model.predict(X_test)\n    \n    # Metriques\n    mae = mean_absolute_error(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    r2 = r2_score(y_test, y_pred)\n    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n    \n    # Stocker resultats\n    results.append({\n        'algorithm': name,\n        'mae': mae,\n        'rmse': rmse,\n        'r2': r2,\n        'mape': mape,\n        'cv_mae': cv_mae,\n        'train_time': train_time,\n        'predictions': y_pred\n    })\n    \n    # Afficher resultats\n    print(f\"MAE:        {mae:.2f} MW\")\n    print(f\"RMSE:       {rmse:.2f} MW\")\n    print(f\"R2:         {r2:.4f}\")\n    print(f\"MAPE:       {mape:.2f}%\")\n    print(f\"CV MAE:     {cv_mae:.2f} MW\")\n    print(f\"Train time: {train_time:.2f}s\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"   BENCHMARKING TERMINE\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. Comparaison des resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# DataFrame des resultats\ndf_results = pd.DataFrame([{\n    'Algorithme': r['algorithm'],\n    'MAE (MW)': round(r['mae'], 2),\n    'RMSE (MW)': round(r['rmse'], 2),\n    'R2': round(r['r2'], 4),\n    'MAPE (%)': round(r['mape'], 2),\n    'CV MAE (MW)': round(r['cv_mae'], 2),\n    'Temps (s)': round(r['train_time'], 2)\n} for r in results])\n\n# Trier par MAE\ndf_results = df_results.sort_values('MAE (MW)')\n\nprint(\"\\n\" + \"=\" * 90)\nprint(\"   COMPARAISON DES ALGORITHMES\")\nprint(\"=\" * 90)\nprint(df_results.to_string(index=False))\nprint(\"=\" * 90)\n\n# Meilleur algorithme\nbest_algorithm = df_results.iloc[0]['Algorithme']\nprint(f\"\\nMeilleur algorithme: {best_algorithm}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 7. Visualisations comparatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# 1. Comparaison MAE\naxes[0, 0].bar(df_results['Algorithme'], df_results['MAE (MW)'], color=['#3498db', '#e74c3c', '#2ecc71'])\naxes[0, 0].set_title('MAE par algorithme (plus bas = meilleur)', fontsize=12, fontweight='bold')\naxes[0, 0].set_ylabel('MAE (MW)')\naxes[0, 0].tick_params(axis='x', rotation=15)\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. Comparaison R2\naxes[0, 1].bar(df_results['Algorithme'], df_results['R2'], color=['#3498db', '#e74c3c', '#2ecc71'])\naxes[0, 1].set_title('R2 Score par algorithme (plus haut = meilleur)', fontsize=12, fontweight='bold')\naxes[0, 1].set_ylabel('R2 Score')\naxes[0, 1].tick_params(axis='x', rotation=15)\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. Comparaison MAPE\naxes[1, 0].bar(df_results['Algorithme'], df_results['MAPE (%)'], color=['#3498db', '#e74c3c', '#2ecc71'])\naxes[1, 0].set_title('MAPE par algorithme (plus bas = meilleur)', fontsize=12, fontweight='bold')\naxes[1, 0].set_ylabel('MAPE (%)')\naxes[1, 0].tick_params(axis='x', rotation=15)\naxes[1, 0].grid(True, alpha=0.3)\n\n# 4. Temps entrainement\naxes[1, 1].bar(df_results['Algorithme'], df_results['Temps (s)'], color=['#3498db', '#e74c3c', '#2ecc71'])\naxes[1, 1].set_title('Temps entrainement par algorithme', fontsize=12, fontweight='bold')\naxes[1, 1].set_ylabel('Temps (secondes)')\naxes[1, 1].tick_params(axis='x', rotation=15)\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('ml/benchmark_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"Graphiques sauvegardes: ml/benchmark_comparison.png\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": "## 8. Predictions vs Reel pour chaque algorithme"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor idx, result in enumerate(results):\n    y_pred = result['predictions']\n    \n    axes[idx].scatter(y_test, y_pred, alpha=0.5, s=20)\n    axes[idx].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n                   'r--', lw=2, label='Prediction parfaite')\n    axes[idx].set_xlabel('Consommation Reelle (MW)')\n    axes[idx].set_ylabel('Consommation Predite (MW)')\n    axes[idx].set_title(f\"{result['algorithm']}\\nR2 = {result['r2']:.3f}\", fontweight='bold')\n    axes[idx].legend()\n    axes[idx].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 9. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 70)\nprint(\"   CONCLUSION BENCHMARKING\")\nprint(\"=\" * 70)\nprint(f\"\\nAlgorithme selectionne: {best_algorithm}\")\nprint(\"\\nJustification:\")\n\nbest_result = [r for r in results if r['algorithm'] == best_algorithm][0]\n\nprint(f\"- MAE:  {best_result['mae']:.2f} MW (erreur moyenne absolue)\")\nprint(f\"- R2:   {best_result['r2']:.4f} (qualite predictions)\")\nprint(f\"- MAPE: {best_result['mape']:.2f}% (erreur en pourcentage)\")\nprint(f\"\\nLe {best_algorithm} offre le meilleur compromis entre:\")\nprint(\"  1. Precision des predictions (MAE et R2)\")\nprint(\"  2. Robustesse (cross-validation)\")\nprint(\"  3. Temps d'entrainement raisonnable\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 10. Export resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "# Sauvegarder resultats\ndf_results.to_csv('ml/benchmark_results.csv', index=False)\nprint(\"Resultats sauvegardes: ml/benchmark_results.csv\")\n\n# Resume\nsummary = {\n    'best_algorithm': best_algorithm,\n    'metrics': {\n        'mae': float(best_result['mae']),\n        'rmse': float(best_result['rmse']),\n        'r2': float(best_result['r2']),\n        'mape': float(best_result['mape'])\n    },\n    'all_results': df_results.to_dict('records')\n}\n\nimport json\nwith open('ml/benchmark_summary.json', 'w') as f:\n    json.dump(summary, f, indent=2)\n\nprint(\"Resume sauvegarde: ml/benchmark_summary.json\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}